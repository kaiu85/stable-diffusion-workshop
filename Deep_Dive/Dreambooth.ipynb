{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiu85/stable-diffusion-workshop/blob/main/Deep_Dive/Dreambooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAZq3vFDcFiT"
      },
      "source": [
        "# Dreambooth fine-tuning for Stable Diffusion using dðŸ§¨ffusers \n",
        "\n",
        "This notebook shows how to \"teach\" Stable Diffusion a new concept via Dreambooth using ðŸ¤— Hugging Face [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers). \n",
        "\n",
        "![Dreambooth Example](https://dreambooth.github.io/DreamBooth_files/teaser_static.jpg)\n",
        "_By using just 3-5 images you can teach new concepts to Stable Diffusion and personalize the model on your own images_ \n",
        "\n",
        "For a general introduction to the Stable Diffusion model please refer to this [notebook](https://colab.research.google.com/github/kaiu85/stable-diffusion-workshop/blob/main/stable_diffusion.ipynb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbzZ9xe6dWwf"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30lu8LWXmg5j"
      },
      "outputs": [],
      "source": [
        "!pip install -qq diffusers==0.4.1 accelerate tensorboard transformers ftfy gradio\n",
        "!pip install -qq \"ipywidgets>=7,<8\"\n",
        "!pip install -qq bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log into your Huggingface account. Reminder: You can find your tokens here: https://huggingface.co/settings/tokens."
      ],
      "metadata": {
        "id": "_QMGTFw7Qv1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24zwrNSBm4A3"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_h0kO-VnQog"
      },
      "outputs": [],
      "source": [
        "#@title Import required libraries\n",
        "import argparse\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "from contextlib import nullcontext\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import PIL\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import set_seed\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl3r7A_3ASxm"
      },
      "source": [
        "## Settings for teaching your new concept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If5Jswe526QP"
      },
      "outputs": [],
      "source": [
        "# We'll be again using the CompVis/stable-diffusion-v1-4 pretrained model\n",
        "pretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the images of the new concept, which you want to teach stable-diffusion, to the Colab instance by opening the file explorer via the ![folder.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABkAAAAhCAYAAAAswACjAAABU2lDQ1BJQ0MgUHJvZmlsZQAAGJV1kD9Lw1AUxU80UrEiio6CGUQqVNFUNxFqh1IUCa3/cUnTmAppfCZREb+Ag4KTk+Dm3EHopDj4DRSVDo6uDkIWLfG+Rm2q+B6X8+NwuBwu0AKVMVMEULJcO5uekVZW16TIC6Lood+OcVVzWFJR5iiCb21+3gMErncjfNflbux8/uD1KL00uz19UvX+5pteR0F3NNIPmpjGbBcQBomVPZdxpkGfTaWIDzkbAZ9xzgdcrmcWsiniW+JuragWiO+J4/mQb4S4ZO5oXx14+07dWsxxpelHBgpykJHA5D+5iXouhS0w7MPGJgwU4UJCkhwGEzpxBhY0jCJOLGOMRub3/X23huc8AVO0Wwzl1geA8hVVPm54Q89AbxdwbTHVVn+uKXiis5GQA45WgLZT339bBiLDQO3R998rvl+7AFqrwI33CRIQYb3DFBkQAAAAVmVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAADkoYABwAAABIAAABEoAIABAAAAAEAAAAZoAMABAAAAAEAAAAhAAAAAEFTQ0lJAAAAU2NyZWVuc2hvdKo2hiwAAAHUaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA2LjAuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPGV4aWY6UGl4ZWxZRGltZW5zaW9uPjMzPC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjI1PC9leGlmOlBpeGVsWERpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6VXNlckNvbW1lbnQ+U2NyZWVuc2hvdDwvZXhpZjpVc2VyQ29tbWVudD4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CsQxSzEAAAC/SURBVEgN7VZRDsMgCLXLLqNeR+Mh9TrqcbbRhB8BV5I12QckxgYePPKKxuP1MXezPW6uf5Y3EpXKJtf/yfWUWhpjkFAIgfiuOFiSUgqbm1JyOWc2tnMSklqriG+tuTmnGPfes00QEqwQY8TPyzs0AXmrrCIJSLOCd2zwD3vv51rz7DDulCMxk4tIsnOQEYY5h3mHcdTYDk9IsDAQwdIad+0c0muFuyC/Ea6HEPEiCQJ+sdsIq1Q0uUwulQIq8BsmcTbMzfQYewAAAABJRU5ErkJggg==)-icon on the left. Then you can upload files using the ![upload.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABsAAAAcCAYAAACQ0cTtAAABU2lDQ1BJQ0MgUHJvZmlsZQAAGJV1kD9Lw1AUxU80UrEiio6CGUQqVNFUNxFqh1IUCa3/cUnTmAppfCZREb+Ag4KTk+Dm3EHopDj4DRSVDo6uDkIWLfG+Rm2q+B6X8+NwuBwu0AKVMVMEULJcO5uekVZW16TIC6Lood+OcVVzWFJR5iiCb21+3gMErncjfNflbux8/uD1KL00uz19UvX+5pteR0F3NNIPmpjGbBcQBomVPZdxpkGfTaWIDzkbAZ9xzgdcrmcWsiniW+JuragWiO+J4/mQb4S4ZO5oXx14+07dWsxxpelHBgpykJHA5D+5iXouhS0w7MPGJgwU4UJCkhwGEzpxBhY0jCJOLGOMRub3/X23huc8AVO0Wwzl1geA8hVVPm54Q89AbxdwbTHVVn+uKXiis5GQA45WgLZT339bBiLDQO3R998rvl+7AFqrwI33CRIQYb3DFBkQAAAAVmVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAADkoYABwAAABIAAABEoAIABAAAAAEAAAAboAMABAAAAAEAAAAcAAAAAEFTQ0lJAAAAU2NyZWVuc2hvdHZ6LUgAAAHUaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA2LjAuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPGV4aWY6UGl4ZWxZRGltZW5zaW9uPjI4PC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjI3PC9leGlmOlBpeGVsWERpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6VXNlckNvbW1lbnQ+U2NyZWVuc2hvdDwvZXhpZjpVc2VyQ29tbWVudD4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkKYPr0AAAEVSURBVEgN7ZZRDoMgDIZx2WnwOhrPSPQ6ep1tv0mXFhBaND4s64NgafvZUti610fcTfK4ibNjfhf25GXcts0ty+LWdeXqZD4MgxvHMdHXFKKMGhACwm6e51rsZF1kRhmFEBJDUkzTtE8BhFgyFJnt3oaHNcNTMHyXBXgaZgFeAiMgxpKIBikZ0lqueahpyOZovCyzIwDXq2E4Vy1ni8NUZQSEzhWcLWeLw6qZxSBLq3MQ5kVYDCLnVmARRsFx8ZLwOem0Y3HPsDd93zvv/XfPuE4LIbtqZgDFktPFNrn3Kizn1KorlpEHzd0cfF0zFzDsD37TtNcPB8C3JqKM6DSNUxwUPpou7f7/G+PStbyLPWsJYPF5A6jwXuPXwH0TAAAAAElFTkSuQmCC)-icon."
      ],
      "metadata": {
        "id": "TCLypgRfRgCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now change the ``image_names`` list in the next cell, so that it contains the names of the uploaded images. For this notebook to run without uploading images, we just download some images from the web and collect their filenames in this list."
      ],
      "metadata": {
        "id": "O8Bs8dxmR_pj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USqTimsBA96M"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/8tw7xtgf3rrintu/kai1.jpeg\n",
        "!wget https://www.dropbox.com/s/0lfixzo87o6mdlc/kai2.jpeg\n",
        "\n",
        "image_names = [\n",
        "'kai1.jpeg',\n",
        "'kai2.jpeg'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell reads in the images, which you've just uploaded and converts them into the right format for the stable-diffusion model."
      ],
      "metadata": {
        "id": "3rfxyDX1UML0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60jVYSk0BGC8"
      },
      "outputs": [],
      "source": [
        "#Setup and check the images you have just added\n",
        "import requests\n",
        "import glob\n",
        "from io import BytesIO\n",
        "\n",
        "images = list(filter(None,[Image.open(image_name).convert(\"RGB\") for image_name in image_names]))\n",
        "save_path = \"./my_concept\"\n",
        "if not os.path.exists(save_path):\n",
        "  os.mkdir(save_path)\n",
        "[image.save(f\"{save_path}/{i}.jpeg\") for i, image in enumerate(images)]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "for image in images:\n",
        "  display(image)"
      ],
      "metadata": {
        "id": "wNn3pe6-cXyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to generate images with the concept, which we are trying to teach the stable-diffusion model, we have to give this concept a **name**.\n",
        "As this name, we use the special token associated with the letters ``sks``. In the next cell, we have to define a prompt, which fits all training images, and in which we also give the model some useful context cues, to **what exactly** ``sks`` is supposed to mean (it could be a person, a cat, a dog, a pet, a toy, an electric device, a car, a motorcycle, ...). As the template-images, which we've put online, all show a portrait photograph of a specific person, we use the prompt ``\"a portrait photo of sks person\"``. For stable-diffusion to learn and understand your own concept, make this prompt as informative as possible."
      ],
      "metadata": {
        "id": "5IjjovztUbk_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i_vLTBxAXpE"
      },
      "outputs": [],
      "source": [
        "#Settings for your newly created concept\n",
        "# `instance_prompt` is a prompt that should contain a good description of what \n",
        "# your object or style is, together with the initializer word `sks`  \n",
        "instance_prompt = \"a portrait photo of sks person\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Prior preservation\" is a technique, which prevents stable-diffusion from forgetting other concepts, while it tries to learn our new concept. It tremendously improves the quality of the generated images. However, it requires more memory than we have available on free Colab instances, so we don't use it today.\n"
      ],
      "metadata": {
        "id": "wMfKiTLsVkwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can't use prior preservation on a free Colab\n",
        "# instance due to memory constraints.\n",
        "prior_preservation = False"
      ],
      "metadata": {
        "id": "Yot48bHPVlGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're running this on a larger instance, you can turn on prior preservation by setting the above variable to 'True'\n",
        "\n",
        "Then you have to specify a prompt, which describes your training images well within the known classes of stable diffusion, i.e. without refering to the 'sks' token.\n"
      ],
      "metadata": {
        "id": "Ymo6PdtnWyFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prior preservation prevents 'overfitting' to your training data, by adding other images, which show the same class of object as your training image, but not the specific instance (i.e., pictures of the same breed of dog, same class of cars, ...).\n",
        "\n",
        "You can either look for such pictures yourself, or just generate them from the stable-diffusion model. For this, you just have to give the model a prompt, which describes the specific class of object, to which the concept on your training images belongs.\n",
        "\n",
        "Again, we cannot run this today, so feel free to just skip the next code cell, which collects all relevant parameters for prior preservation, just for the sake of completeness."
      ],
      "metadata": {
        "id": "2DYkJFwmXBl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prior_preservation_class_prompt = \"portrait photo of a man with glasses and brown hair\"\n",
        "\n",
        "num_class_images = 12 \n",
        "sample_batch_size = 2\n",
        "prior_loss_weight = 0.5\n",
        "prior_preservation_class_folder = \"./class_images\"\n",
        "class_data_root=prior_preservation_class_folder\n",
        "class_prompt=prior_preservation_class_prompt\n",
        "num_class_images = 12 \n",
        "sample_batch_size = 2\n",
        "\n",
        "# `prior_preservation_weight` determins how strong the class for \n",
        "# prior preservation should be \n",
        "prior_loss_weight = 1\n",
        "\n",
        "\n",
        "# If the `prior_preservation_class_folder` is empty, images for the \n",
        "# class will be generated with the class prompt. \n",
        "# Otherwise, fill this folder with images of items on the same class as \n",
        "# your concept (but not images of the concept itself)\n",
        "prior_preservation_class_folder = \"./class_images\" #\n",
        "class_data_root=prior_preservation_class_folder"
      ],
      "metadata": {
        "id": "2jb-epZQXBu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D633UIuGgs6M"
      },
      "source": [
        "## Teach the model the new concept (fine-tuning with Dreambooth)\n",
        "Execute this this sequence of cells to run the training process. The whole process may take from 15 min to 2 hours. (Open this block if you are interested in how this process works under the hood or if you want to change advanced training settings or hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lzxceJuASiev"
      },
      "outputs": [],
      "source": [
        "#@title Setup the Classes\n",
        "from pathlib import Path\n",
        "from torchvision import transforms\n",
        "\n",
        "class DreamBoothDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        instance_data_root,\n",
        "        instance_prompt,\n",
        "        tokenizer,\n",
        "        class_data_root=None,\n",
        "        class_prompt=None,\n",
        "        size=512,\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.instance_data_root = Path(instance_data_root)\n",
        "        if not self.instance_data_root.exists():\n",
        "            raise ValueError(\"Instance images root doesn't exists.\")\n",
        "\n",
        "        self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "        self.instance_prompt = instance_prompt\n",
        "        self._length = self.num_instance_images\n",
        "\n",
        "        if class_data_root is not None:\n",
        "            self.class_data_root = Path(class_data_root)\n",
        "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
        "            self.class_images_path = list(Path(class_data_root).iterdir())\n",
        "            self.num_class_images = len(self.class_images_path)\n",
        "            self._length = max(self.num_class_images, self.num_instance_images)\n",
        "            self.class_prompt = class_prompt\n",
        "        else:\n",
        "            self.class_data_root = None\n",
        "\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
        "        if not instance_image.mode == \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            self.instance_prompt,\n",
        "            padding=\"do_not_pad\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "        ).input_ids\n",
        "\n",
        "        if self.class_data_root:\n",
        "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
        "            if not class_image.mode == \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                self.class_prompt,\n",
        "                padding=\"do_not_pad\",\n",
        "                truncation=True,\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "            ).input_ids\n",
        "        \n",
        "        return example\n",
        "\n",
        "\n",
        "class PromptDataset(Dataset):\n",
        "    def __init__(self, prompt, num_samples):\n",
        "        self.prompt = prompt\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        example[\"prompt\"] = self.prompt\n",
        "        example[\"index\"] = index\n",
        "        return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mneZ4Ct2BenE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generate Class Images\n",
        "import gc\n",
        "if(prior_preservation):\n",
        "    class_images_dir = Path(class_data_root)\n",
        "    if not class_images_dir.exists():\n",
        "        class_images_dir.mkdir(parents=True)\n",
        "    cur_class_images = len(list(class_images_dir.iterdir()))\n",
        "\n",
        "    if cur_class_images < num_class_images:\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            pretrained_model_name_or_path, revision=\"fp16\", torch_dtype=torch.float16\n",
        "        ).to(\"cuda\")\n",
        "        pipeline.enable_attention_slicing()\n",
        "        pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "        num_new_images = num_class_images - cur_class_images\n",
        "        print(f\"Number of class images to sample: {num_new_images}.\")\n",
        "\n",
        "        sample_dataset = PromptDataset(class_prompt, num_new_images)\n",
        "        sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=sample_batch_size)\n",
        "\n",
        "        for example in tqdm(sample_dataloader, desc=\"Generating class images\"):\n",
        "            images = pipeline(example[\"prompt\"]).images\n",
        "\n",
        "            for i, image in enumerate(images):\n",
        "                image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n",
        "        pipeline = None\n",
        "        gc.collect()\n",
        "        del pipeline\n",
        "        with torch.no_grad():\n",
        "          torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Stable Diffusion model\n",
        "Please read and if you agree accept the LICENSE [here](https://huggingface.co/CompVis/stable-diffusion-v1-4) if you see an error"
      ],
      "metadata": {
        "id": "yeWTCjeOYkNo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIFaJum5nqeo"
      },
      "outputs": [],
      "source": [
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
        ")\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"vae\"\n",
        ")\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"unet\"\n",
        ")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    subfolder=\"tokenizer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we set up the training parameters. One of the most crucial parameters is the number of training steps. It should not be too few (150 likely are too few), so that stable-diffusion can learn the new concept, but it should also not be too many, so that stable-diffusion does not overfit to the new concept and starts to forget other concepts (600 likely are too many). So feel free to experiment with the ``max_train_steps``parameter."
      ],
      "metadata": {
        "id": "Qi0Duhs_msLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4ayDcwEHDa4"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "args = Namespace(\n",
        "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "    resolution=512,\n",
        "    center_crop=True,\n",
        "    instance_data_dir=save_path,\n",
        "    instance_prompt=instance_prompt,\n",
        "    learning_rate=5e-06,\n",
        "    max_train_steps=450,\n",
        "    train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    max_grad_norm=1.0,\n",
        "    mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
        "    gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
        "    use_8bit_adam=True, # use 8bit optimizer from bitsandbytes\n",
        "    seed=3434554,\n",
        "    with_prior_preservation=prior_preservation, \n",
        "    prior_loss_weight=prior_loss_weight,\n",
        "    sample_batch_size=2,\n",
        "    class_data_dir=prior_preservation_class_folder, \n",
        "    class_prompt=prior_preservation_class_prompt, \n",
        "    num_class_images=num_class_images, \n",
        "    output_dir=\"dreambooth-concept\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lKGmcIyJbCu"
      },
      "outputs": [],
      "source": [
        "#@title Training function\n",
        "from accelerate.utils import set_seed\n",
        "def training_function(text_encoder, vae, unet):\n",
        "    logger = get_logger(__name__)\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        mixed_precision=args.mixed_precision,\n",
        "    )\n",
        "\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "        unet.enable_gradient_checkpointing()\n",
        "\n",
        "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
        "    if args.use_8bit_adam:\n",
        "        optimizer_class = bnb.optim.AdamW8bit\n",
        "    else:\n",
        "        optimizer_class = torch.optim.AdamW\n",
        "\n",
        "    optimizer = optimizer_class(\n",
        "        unet.parameters(),  # only optimize unet\n",
        "        lr=args.learning_rate,\n",
        "    )\n",
        "\n",
        "    noise_scheduler = DDPMScheduler(\n",
        "        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n",
        "    )\n",
        "    \n",
        "    train_dataset = DreamBoothDataset(\n",
        "        instance_data_root=args.instance_data_dir,\n",
        "        instance_prompt=args.instance_prompt,\n",
        "        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
        "        class_prompt=args.class_prompt,\n",
        "        tokenizer=tokenizer,\n",
        "        size=args.resolution,\n",
        "        center_crop=args.center_crop,\n",
        "    )\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "\n",
        "        # concat class and instance examples for prior preservation\n",
        "        if args.with_prior_preservation:\n",
        "            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
        "            pixel_values += [example[\"class_images\"] for example in examples]\n",
        "\n",
        "        pixel_values = torch.stack(pixel_values)\n",
        "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "\n",
        "        input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        batch = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"pixel_values\": pixel_values,\n",
        "        }\n",
        "        return batch\n",
        "    \n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
        "\n",
        "    # Move text_encode and vae to gpu\n",
        "    text_encoder.to(accelerator.device)\n",
        "    vae.to(accelerator.device)\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "  \n",
        "    # Train!\n",
        "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "    # Only show the progress bar once on each machine.\n",
        "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    progress_bar.set_description(\"Steps\")\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(num_train_epochs):\n",
        "        unet.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Convert images to latent space\n",
        "                with torch.no_grad():\n",
        "                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
        "                    latents = latents * 0.18215\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn(latents.shape).to(latents.device)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\n",
        "                ).long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                with torch.no_grad():\n",
        "                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                # Predict the noise residual\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                if args.with_prior_preservation:\n",
        "                    # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\n",
        "                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
        "                    noise, noise_prior = torch.chunk(noise, 2, dim=0)\n",
        "\n",
        "                    # Compute instance loss\n",
        "                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                    # Compute prior loss\n",
        "                    prior_loss = F.mse_loss(noise_pred_prior, noise_prior, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                    # Add the prior loss to the instance loss.\n",
        "                    loss = loss + args.prior_loss_weight * prior_loss\n",
        "                else:\n",
        "                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "            logs = {\"loss\": loss.detach().item()}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "\n",
        "            if global_step >= args.max_train_steps:\n",
        "                break\n",
        "\n",
        "        accelerator.wait_for_everyone()\n",
        "    \n",
        "    # Create the pipeline using using the trained modules and save it.\n",
        "    if accelerator.is_main_process:\n",
        "        pipeline = StableDiffusionPipeline(\n",
        "            text_encoder=text_encoder,\n",
        "            vae=vae,\n",
        "            unet=accelerator.unwrap_model(unet),\n",
        "            tokenizer=tokenizer,\n",
        "            scheduler=PNDMScheduler(\n",
        "                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n",
        "            ),\n",
        "            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
        "            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
        "        )\n",
        "        pipeline.save_pretrained(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfeKJn_LJi_V"
      },
      "outputs": [],
      "source": [
        "#@title Run training\n",
        "import accelerate\n",
        "accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50JuJUM8EG1h"
      },
      "source": [
        "## Run the code with your newly trained model\n",
        "If you have just trained your model with the code above, use the block below to run it.\n",
        "\n",
        "Also explore the [DreamBooth Concepts Library](https://huggingface.co/sd-dreambooth-library) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CMlPbOeEC09",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Set up the pipeline \n",
        "try:\n",
        "    pipe\n",
        "except NameError:\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        args.output_dir,\n",
        "        torch_dtype=torch.float16,\n",
        "    ).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3UREGd7EkLh"
      },
      "outputs": [],
      "source": [
        "#@title Run the Stable Diffusion pipeline on Colab\n",
        "\n",
        "# IMPORTANT: Don't forget to use the 'sks' placeholder token in your prompt\n",
        "prompt = \"oil portrait of sks person\"\n",
        "\n",
        "num_samples = 2 # how many samples should be generated simultaneously\n",
        "num_rows = 2 # how often should this number of samples be generated\n",
        "\n",
        "all_images = [] \n",
        "for _ in range(num_rows):\n",
        "    images = pipe(prompt, num_images_per_prompt=num_samples, num_inference_steps=50, guidance_scale=7.5).images\n",
        "    all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, num_samples, num_rows)\n",
        "grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mO9y6YWwCit"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('3.7.9')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "76721e0cd9246c299eb22246d1f3c601ec1aef6bd84d45d2547549094e7b6fb7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}